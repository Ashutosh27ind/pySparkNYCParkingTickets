{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Author : Ashutosh Kumar\n",
    "### Contact : ashutoshind2017@outlook.com\n",
    "### Kernel: PySpark \n",
    "### Document Name: NYC Parking Tickets Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Big data analytics allows you to analyse data at scale. It has applications in almost every industry in the world. Let’s consider an unconventional application that you wouldn’t ordinarily encounter.\n",
    "\n",
    "New York City is a thriving metropolis. Just like most other metros its size, one of the biggest problems its citizens face is parking. The classic combination of a huge number of cars and cramped geography leads to a huge number of parking tickets.\n",
    "In an attempt to scientifically analyse this phenomenon, the NYC Police Department has collected data for parking tickets. Of these, the data files for multiple years are publicly available on Kaggle. We will try and perform some exploratory analysis on a part of this data. Spark will allow us to analyse the full files at high speeds as opposed to taking a series of random samples that will approximate the population. For the scope of this analysis, we will analyse the parking tickets over the year 2017.\n",
    "Note: Although the broad goal of any analysis of this type is to have better parking and fewer tickets, we are not looking for recommendations on how to reduce the number of parking tickets—there are no specific points reserved for this.\n",
    " The purpose of this assignment is to conduct an exploratory data analysis that will help you understand the data. Since the size of the dataset is large, your queries will take some time to run, and you will need to identify the correct queries quicker.\n",
    "The dataset structure is available on this page along with the data.\n",
    "https://www.kaggle.com/new-york-city/nyc-parking-tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the Dataset\n",
    "The data for this assignment has been placed in HDFS at the following path:\n",
    "#### '/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv'\n",
    " \n",
    " \n",
    "The analysis should be performed on PySpark mounted on your CoreStack cluster, using the PySpark library. Remember that we need to summarise the analysis with insights along with the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Understanding, Visualisation and Preparation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the spark session :\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySpark ML NYC Tickets Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data:\n",
    "\n",
    "#Create dataframe by calling read() method on SparkSession/spark object:\n",
    "# ( used option header true to ignore the first row in csv which is header)\n",
    "\n",
    "NYCParkingdf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "             .load(\"/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the data is loaded in the dataframe :\n",
    "\n",
    "NYCParkingdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Examining the dataset imported:\n",
    "\n",
    "#DataFame will have columns, and we use a schema to define them.\n",
    "# printSchema returns schema in tree format\n",
    "NYCParkingdf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the number of records and lenght of dataframe:\n",
    "print((NYCParkingdf.count(), len(NYCParkingdf.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we have huge data of 10803028 rows and 10 columns in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required functions for finding date range for the tickets:\n",
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "NYCParkingdf.select(min(\"Issue Date\"), max(\"Issue Date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we have data from past 1972 till future date 2069 , but we need to filter the records only for year 2017:\n",
    "\n",
    "# For filtering the data based on the year , lets create a new column in dataframe for the year from Issue Date :\n",
    "\n",
    "from pyspark.sql.functions import col, year\n",
    "\n",
    "NYCParkingdf2 = NYCParkingdf.withColumn(\"Year\", year(col(\"Issue Date\")))\n",
    "NYCParkingdf2.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now filtering the data only for the year 2017 as per the requirement :\n",
    "\n",
    "NYCParkingdf3 = NYCParkingdf2.filter(NYCParkingdf2.Year == \"2017\")\n",
    "NYCParkingdf3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the total number of tickets for the year :\n",
    "\n",
    "# Register the PySpark dataframe as sql temp table for analyis:\n",
    "NYCParkingdf3.createOrReplaceTempView(\"dfNYCTable\")\n",
    "\n",
    "# After registering temp table we can run sql queries:\n",
    "spark.sql('SELECT count(distinct `Summons Number`) FROM dfNYCTable').show()\n",
    "\n",
    "# Used back tick as escape character for the column \"Summons Number\" as it was having spaces between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the number of unique states from where the cars that got parking tickets came:\n",
    "# Registration State column provided this information in the dataframe:\n",
    "\n",
    "spark.sql('SELECT `Registration State`,count(*) as count1 FROM dfNYCTable group by `Registration State` order by count1 desc').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYCParkingdf3.select('Registration State').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the NY state has the highest number of tickets 4273951 and state FO has lowest number of tickets 8\n",
    "# Also there are total 65 states data that is present.\n",
    "\n",
    "# We can see that there are erronous data where the Registration State = 99 , which is incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing state 99 with the state having the maximum entries\n",
    "\n",
    "# Importing required sql function:\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "NYCParkingdf4 = NYCParkingdf3.withColumn(\"Registration State\", \\\n",
    "              when(NYCParkingdf3[\"Registration State\"] == 99, 'NY').otherwise(NYCParkingdf3[\"Registration State\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have replaced the state 99 with the state having highest ticket, lets check the count again:\n",
    "NYCParkingdf4.select('Registration State').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, now we have 64 distinct state and data is cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How often does each violation code occur? Display the frequency of the top five violation codes.\n",
    "\n",
    "violation_code = NYCParkingdf4.select('Violation Code').distinct().count()\n",
    "violation_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we have 100 different violation codes:\n",
    "\n",
    "# Re-Register the PySpark dataframe as sql temp table for analyis:\n",
    "NYCParkingdf4.createOrReplaceTempView(\"dfNYCTable\")\n",
    "\n",
    "# After registering temp table we can run sql queries:\n",
    "spark.sql(\"SELECT `Violation Code`,count(*) as violation_count FROM dfNYCTable group by `Violation Code` order by\\\n",
    "           violation_count desc limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top 5 violation codes are 21,36,38,14 and 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How often does each 'vehicle body type' get a parking ticket? \n",
    "\n",
    "spark.sql(\"SELECT `Vehicle Body Type`,count(*) as body_count FROM dfNYCTable group by `Vehicle Body Type` order by\\\n",
    "           body_count desc limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the top 5 vehicle body type fined are SUBN,4DSD,VAN, DELV and SDN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about the 'vehicle make'? \n",
    "\n",
    "spark.sql(\"SELECT `Vehicle Make`,count(*) as vehicle_count FROM dfNYCTable group by `Vehicle Make` order by\\\n",
    "           vehicle_count desc limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the top 5 vehicle makes which were fined are FORD,TOYOT,HONDA, NISSAN and CHEVR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A precinct is a police station that has a certain zone of the city under its command.\n",
    "# 'Violation Precinct' (This is the precinct of the zone where the violation occurred)\n",
    "# Find top 5 violation precinct :\n",
    "\n",
    "spark.sql(\"SELECT `Violation Precinct`,count(*) as VP_count FROM dfNYCTable group by `Violation Precinct` order by\\\n",
    "           VP_count desc limit 6\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The violation precinct of 0 is invalid and hence top Violation Precinct are 19,13,1,18 and 114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Issuer Precinct' (This is the precinct that issued the ticket.)\n",
    "# Find top 5 violation precinct :\n",
    "\n",
    "spark.sql(\"SELECT `Issuer Precinct`,count(*) as IP_count FROM dfNYCTable group by `Issuer Precinct` order by\\\n",
    "           IP_count desc limit 6\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The violation precinct of 0 is invalid and hence top Violation Precinct are 19,13,1,18 and 114\n",
    "\n",
    "# We can infer from this data that the violation and issuing of ticket is happening in the same zone/precint.\n",
    "# This suggests that the ticketing system might be automated/fast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the violation code frequencies for three precincts that have issued the most number of tickets. Do these precinct zones\n",
    "#have an exceptionally high frequency of certain violation codes? Are these codes common across precincts? \n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark.sql(\"SELECT `Violation Code`,count(*) as violation_count FROM dfNYCTable where `Issuer Precinct` in (19,13,1) \\\n",
    "            group by `Violation Code` order by violation_count desc limit 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top 5 violation codes are for the top 3 precints zone are 14,46,38,37 and 21.\n",
    "# The top 5 violation codes are 21,36,38,14 and 20.\n",
    "\n",
    "# We can infer that we have more traffic violation of the code 46 and 37 in the top 3 fined zones which is not the case with \n",
    "# the data for all the traffic zones.\n",
    "# But we do see few common violation codes which are 21, 36 and 38 there for these top 3 fined zones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the properties of parking violations across different times of the day:\n",
    "\n",
    "# Let's try to find the details on the 'Violation Time':\n",
    "\n",
    "spark.sql(\"SELECT count(1) as null_count FROM dfNYCTable where `Violation Time` IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We dont have any data with NULL for the year 2017 where the violation time is not recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Violation Time field is specified in a strange format. Find a way to make this a time attribute that you can use to\n",
    "# divide into groups. \n",
    "# The time is of datatype string.\n",
    "\n",
    "# In order to retrieve the time of the day in AM/PM, we must use hhmma. But in SimpleDateFormat, a catches AM or PM, \n",
    "# and not A or P. So we need to change our string :\n",
    "\n",
    "import pyspark.sql.functions as F \n",
    "\n",
    "df2 = NYCParkingdf4.withColumn('Violation Time', F.concat(F.col('Violation Time'), F.lit('M')))\n",
    "\n",
    "# We have used the format of hour as K for hour in am/pm (0-11) according to the below Oracle JAVA documentation :\n",
    "# https://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html\n",
    "df3 = df2.withColumn('ts', F.to_timestamp('Violation Time',format='KKmma'))\n",
    "\n",
    "df4 = df3.withColumn('time', F.date_format(F.col('ts'), format='HH:mm'))\n",
    "df4[['Violation Time','time','ts']].show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns Violation Time and Year as it is not needed anymore:\n",
    "\n",
    "# Remove the column\n",
    "#df4=df4.drop(\"Violation Time\",\"Year\").columns\n",
    "\n",
    "columns_to_drop = ['Violation Time', 'Year','ts']\n",
    "df4 = df4.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating unecessary columns have been dropped successfully:\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing column types of time from string to date:\n",
    "\n",
    "# TimestampType()\n",
    "df4 = df4.withColumn(\"time\", col(\"time\").cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column from time to time_stamp:\n",
    "df4 = df4.withColumnRenamed(\"time\", \"time_stamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide 24 hours into six equal discrete bins of time. Choose the intervals as you see fit. For each of these groups, find the\n",
    "# three most commonly occurring violations.\n",
    "\n",
    "# Six equal bins means 240 minutes/4 Hours each:\n",
    "\n",
    "#import org.apache.spark.sql.functions.{col,hour,minute,second}\n",
    "from pyspark.sql.functions import col,hour,minute,second\n",
    "\n",
    "# Re-register the dataframe as sql view:\n",
    "df4.createOrReplaceTempView(\"dfNYCTable\")\n",
    "\n",
    "spark.sql(\"\"\" SELECT CASE when hour(time_stamp) BETWEEN 0 and 3 then '1' \n",
    "                          when hour(time_stamp) BETWEEN 4 and 7 then '2' \n",
    "                          when hour(time_stamp) BETWEEN 8 and 11 then '3' \n",
    "                          when hour(time_stamp) BETWEEN 12 and 15 then '4' \n",
    "                          when hour(time_stamp) BETWEEN 16 and 19 then '5' \n",
    "                          ELSE '6' \n",
    "                      END \n",
    "                          as day_bin, count(*) as bin_count \n",
    "                             FROM dfNYCTable group by day_bin order by bin_count desc \"\"\").show()\n",
    "\n",
    "# We can use triple quotes \"\"\"\"\"\" for multiline SQL statements in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most number of violations are happening for time between 8:00 AM and 11:59 AM i.e peak office commute time/morning\n",
    "# This is followed by violations in evenings between 4:00 PM and 7:59 PM\n",
    "# The least violation time frame is between midnight to 2:59 AM night time which makes sense as its night time, less traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, try another direction. For the three most commonly occurring violation codes, find the most common time of the day \n",
    "# (in terms of the bins from the previous part).\n",
    "\n",
    "# We know from previuos analysis that the three most common violation codes are 21,36 and 38\n",
    "\n",
    "spark.sql(\"\"\" SELECT CASE when hour(time_stamp) BETWEEN 0 and 3 then '1' \n",
    "                          when hour(time_stamp) BETWEEN 4 and 7 then '2' \n",
    "                          when hour(time_stamp) BETWEEN 8 and 11 then '3' \n",
    "                          when hour(time_stamp) BETWEEN 12 and 15 then '4' \n",
    "                          when hour(time_stamp) BETWEEN 16 and 19 then '5' \n",
    "                          ELSE '6' \n",
    "                      END \n",
    "                          as day_bin, count(*) as bin_count \n",
    "                             FROM dfNYCTable \n",
    "                             WHERE `Violation Code` in (21,36,38)\n",
    "                             group by day_bin order by bin_count desc \"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1122797/371383 ,2163554/1329942)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data for the most number of the violations for part of day for top 3 violations type matches with overall violations data\n",
    "# The only difference is violation in morning between 8 AM and 11:59 AM is much higher (3 times) as opposed to 1.5 times for \n",
    "# the top 3 violation types as opposed to overall violation types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s try and find some seasonality in this data:\n",
    "\n",
    "# First, divide the year into a certain number of seasons, and find the frequencies of tickets for each season. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the meteorological definition, the seasons begin on the first day of the months that include the equinoxes and solstices:\n",
    "Source : https://www.timeanddate.com/calendar/aboutseasons.html\n",
    "\n",
    "Spring runs from March 1 to May 31;\n",
    "Summer runs from June 1 to August 31;\n",
    "Fall (autumn) runs from September 1 to November 30; and\n",
    "Winter runs from December 1 to February 28 (February 29 in a leap year).\n",
    "\n",
    "So lets have below seasons for all 12 months: \n",
    "Spring(March, April, May)\n",
    "Summer(June, July, August)\n",
    "Fall(September, October, November)\n",
    "Winter(December, January, February)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" SELECT CASE when month(`Issue Date`) IN (3,4,5) then 'Spring' \n",
    "                          when month(`Issue Date`) IN (6,7,8) then 'Summer' \n",
    "                          when month(`Issue Date`) IN (9,10,11) then 'Fall' \n",
    "                          ELSE 'Winter' \n",
    "                      END \n",
    "                          as Season, count(*) as bin_count \n",
    "                             FROM dfNYCTable group by Season order by bin_count desc \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Spring season records for most number of the violations 2873383 followed by the Winter season 1704690.\n",
    "# Surprisingly, the count of traffic tickets are very low 979 and least among all the seasons for Fall season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the three most common violations for each of these seasons:\n",
    "\n",
    "spark.sql(\"\"\" SELECT `Violation Code`,count(*) as violation_count, \"Spring\" as Season\n",
    "                           FROM dfNYCTable where month(`Issue Date`) IN (3,4,5)\n",
    "                             group by `Violation Code` order by violation_count desc limit 3\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the three most common violations for each of these seasons:\n",
    "\n",
    "spark.sql(\"\"\" SELECT `Violation Code`,count(*) as violation_count, \"Summer\" as Season\n",
    "                           FROM dfNYCTable where month(`Issue Date`) IN (6,7,8)\n",
    "                             group by `Violation Code` order by violation_count desc limit 3\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the three most common violations for each of these seasons:\n",
    "\n",
    "spark.sql(\"\"\" SELECT `Violation Code`,count(*) as violation_count, \"Fall\" as Season\n",
    "                           FROM dfNYCTable where month(`Issue Date`) IN (9,10,11)\n",
    "                             group by `Violation Code` order by violation_count desc limit 3\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the three most common violations for each of these seasons:\n",
    "\n",
    "spark.sql(\"\"\" SELECT `Violation Code`,count(*) as violation_count, \"Winter\" as Season\n",
    "                           FROM dfNYCTable where month(`Issue Date`) IN (12,1,2)\n",
    "                             group by `Violation Code` order by violation_count desc limit 3\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top 3 violation code remains same for the Summer, Winter and Spring season which are 21, 36 and 38.\n",
    "# On the other hand top 3 violation code for the Fall month are 46,21 and 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fines collected from all the instances of parking violation constitute a source of revenue for the NYC Police Department. Let’s take an example of estimating this for the three most commonly occurring codes:\n",
    "\n",
    "#### Find the total occurrences of the three most common violation codes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT count(1) as violation_count FROM dfNYCTable where \\\n",
    "             `Violation Code` in ('21','36','38') \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The total occurence of three most common violation code for the year 2017 is 1972931"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, visit the website:\n",
    "http://www1.nyc.gov/site/finance/vehicles/services-violation-codes.page\n",
    "It lists the fines associated with different violation codes. They’re divided into two categories: one for the highest-density locations in the city and the other for the rest of the city. For the sake of simplicity, take the average of the two.\n",
    "Using this information, find the total amount collected for the three violation codes with the maximum tickets. State the code that has the highest total collection.\n",
    "What can you intuitively infer from these findings?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 3 fines and correponding calculations :\n",
    "(Calculating average of two demographics for each fine type)\n",
    "\n",
    "21 \tStreet Cleaning: No parking where parking is not allowed by sign, street marking or traffic control device ($65, $45)\n",
    "    Resultant fine = $55\n",
    "36 \tExceeding the posted speed limit in or near a designated school zone. ($50,$50)\n",
    "    Resultant fine = $50\n",
    "38  Failing to show a receipt or tag in the windshield. Drivers get a 5-minute grace period past the expired time on parking         meter receipts. ($65, $35)\n",
    "    Resultant fine = $50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a new PySpark dataframe to update the fine amount :\n",
    "\n",
    "df5 = df4.withColumn(\n",
    "    'fine_amount',\n",
    "    F.when((F.col(\"Violation Code\") == '21') , 55)\\\n",
    "    .when((F.col(\"Violation Code\") == '36') , 50)\\\n",
    "    .when((F.col(\"Violation Code\") == '38') , 50)\\\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "df5.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's rebuild the sql view from new dataframe:\n",
    "\n",
    "df5.createOrReplaceTempView(\"dfNYCTable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not using the filter condition here as we want total fine amount for top 3 violations and dataframe was updated only for top 3:\n",
    "spark.sql(\"\"\" SELECT sum(fine_amount) as violation_amount_total\n",
    "                           FROM dfNYCTable \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The total amount collected for the three violation codes with the maximum tickets is 102486985 $ for year 2017 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" SELECT `Violation Code`, sum(fine_amount) as violation_amount_total\n",
    "                           FROM dfNYCTable \n",
    "                           group by `Violation Code` order by violation_amount_total desc limit 3 \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, the code that has the highest total collection is 21 from traffic tickets amounting to whopping 42244785 $.\n",
    "# The traffic violation code 21 means fine due to violation of \"No Parking\" rule.\n",
    "\n",
    "# We can infer that the US cities alike other cities of world are facing issue with the Street Cleaning and this also results\n",
    "# in the more traffic congestions and higher commute time for city dwellers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
